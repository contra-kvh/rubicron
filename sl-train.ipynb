{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98834b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import chess\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.LEARNING_RATE = 0.02\n",
    "        self.CONV_FILTERS = 256\n",
    "        self.NUM_RESIDUAL = 20\n",
    "        self.INPUT_SHAPE = (19, 8, 8)\n",
    "        self.OUTPUT_SHAPE = (4672, 1)\n",
    "        self.DEVICE = torch.device(\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.SIMULATIONS_PER_MOVE = 200\n",
    "        self.DIRICHLET_NOISE = 0.25\n",
    "\n",
    "\n",
    "config = Config()\n",
    "print(f\"Using device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def residual_block(channels):\n",
    "    return nn.Sequential(\n",
    "        conv_block(channels, channels),\n",
    "        nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(channels),\n",
    "    )\n",
    "\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                input_channels, 73, kernel_size=1\n",
    "            ),  # Matches saved shape (73, 256, 1, 1)\n",
    "            nn.BatchNorm2d(73),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(73 * config.INPUT_SHAPE[1] * config.INPUT_SHAPE[2], output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                config.INPUT_SHAPE[1] * config.INPUT_SHAPE[2], 512\n",
    "            ),  # Matches saved shape (512, 64)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, output_size),  # Extra layer (head.7 in state_dict)\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class RLModel(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        c, _, _ = input_shape\n",
    "        self.conv1 = conv_block(c, config.CONV_FILTERS)\n",
    "        self.residuals = nn.Sequential(\n",
    "            *[residual_block(config.CONV_FILTERS) for _ in range(config.NUM_RESIDUAL)]\n",
    "        )\n",
    "        self.policy_head = PolicyHead(config.CONV_FILTERS, output_shape[0])\n",
    "        self.value_head = ValueHead(config.CONV_FILTERS, output_shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for res in self.residuals:\n",
    "            x = F.relu(res(x) + x)\n",
    "        return self.policy_head(x), self.value_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, hdf5_path):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.file = None  # lazy loading\n",
    "\n",
    "        # Use a temporary handle to fetch length info\n",
    "        with h5py.File(hdf5_path, 'r') as f:\n",
    "            self.num_samples = f['inputs'].shape[0]\n",
    "\n",
    "    def _lazy_init(self):\n",
    "        if self.file is None:\n",
    "            self.file = h5py.File(self.hdf5_path, 'r')\n",
    "            self.inputs = self.file['inputs']\n",
    "            self.policies = self.file['policies']\n",
    "            self.values = self.file['values']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._lazy_init()\n",
    "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.float32)     # (19, 8, 8)\n",
    "        policy_tensor = torch.tensor(self.policies[idx], dtype=torch.float32)  # (73, 8, 8)\n",
    "        value_tensor = torch.tensor(self.values[idx], dtype=torch.float32)     # (1,)\n",
    "        policy_target = policy_tensor.flatten().argmax()  # class index\n",
    "        return input_tensor, policy_target, value_tensor\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.file is not None:\n",
    "            self.file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdc84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.002\n",
    "NUM_EPOCHS = 6\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LOG_INTERVAL = 100\n",
    "SAVE_PATH = \"latest-trained.pth\"\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, hdf5_path, save_path=SAVE_PATH):\n",
    "    # Device setup\n",
    "    device = config.DEVICE\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Training for {NUM_EPOCHS} epochs\")\n",
    "\n",
    "    criterion_policy = nn.CrossEntropyLoss()\n",
    "    criterion_value = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "    # Dataset and data loaders\n",
    "    dataset = ChessDataset(hdf5_path)\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=SubsetRandomSampler(train_indices),\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=SubsetRandomSampler(val_indices),\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Training samples: {len(train_indices)}, Validation samples: {len(val_indices)}\"\n",
    "    )\n",
    "\n",
    "    # Training history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_policy_loss\": [],\n",
    "        \"train_value_loss\": [],\n",
    "        \"train_policy_acc\": [],\n",
    "        \"train_policy_top3_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_policy_loss\": [],\n",
    "        \"val_value_loss\": [],\n",
    "        \"val_policy_acc\": [],\n",
    "        \"val_policy_top3_acc\": [],\n",
    "    }\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} [{'='*20}]\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_policy_loss, train_value_loss = 0, 0, 0\n",
    "        train_correct, train_top3_correct, train_total = 0, 0, 0\n",
    "        train_value_min, train_value_max = float(\"inf\"), float(\"-inf\")\n",
    "\n",
    "        for batch_idx, (inputs, policy_targets, value_targets) in enumerate(\n",
    "            train_loader\n",
    "        ):\n",
    "            inputs = inputs.to(device, non_blocking=True)  # (batch, 19, 8, 8)\n",
    "            policy_targets = policy_targets.to(\n",
    "                device, non_blocking=True, dtype=torch.long\n",
    "            )  # (batch,)\n",
    "            value_targets = value_targets.to(device, non_blocking=True)  # (batch, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            policy_pred, value_pred = model(inputs)  # (batch, 73*8*8), (batch, 1)\n",
    "\n",
    "            policy_loss = criterion_policy(policy_pred, policy_targets)\n",
    "            value_loss = criterion_value(value_pred.squeeze(), value_targets.squeeze())\n",
    "            loss = 0.4 * policy_loss + 0.6 * value_loss\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_policy_loss += policy_loss.item()\n",
    "            train_value_loss += value_loss.item()\n",
    "\n",
    "            # Policy accuracy (top-1 and top-3)\n",
    "            _, predicted = torch.max(policy_pred, 1)\n",
    "            train_correct += (predicted == policy_targets).sum().item()\n",
    "            _, top3_predicted = torch.topk(policy_pred, k=3, dim=1)\n",
    "            train_top3_correct += sum(\n",
    "                policy_targets[i] in top3_predicted[i]\n",
    "                for i in range(policy_targets.size(0))\n",
    "            )\n",
    "            train_total += policy_targets.size(0)\n",
    "\n",
    "            # Track value prediction range\n",
    "            train_value_min = min(train_value_min, value_pred.min().item())\n",
    "            train_value_max = max(train_value_max, value_pred.max().item())\n",
    "\n",
    "            if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "                print(\n",
    "                    f\"Batch {batch_idx+1}/{len(train_loader)}: \"\n",
    "                    f\"Loss={loss.item():.4f}, Policy Loss={policy_loss.item():.4f}, \"\n",
    "                    f\"Value Loss={value_loss.item():.4f}, \"\n",
    "                    f\"Top-1 Acc={100 * train_correct/train_total:.2f}%, \"\n",
    "                    f\"Top-3 Acc={100 * train_top3_correct/train_total:.2f}%\"\n",
    "                )\n",
    "\n",
    "        # Epoch metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_policy_loss /= len(train_loader)\n",
    "        train_value_loss /= len(train_loader)\n",
    "        train_policy_acc = 100 * train_correct / train_total\n",
    "        train_policy_top3_acc = 100 * train_top3_correct / train_total\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_policy_loss\"].append(train_policy_loss)\n",
    "        history[\"train_value_loss\"].append(train_value_loss)\n",
    "        history[\"train_policy_acc\"].append(train_policy_acc)\n",
    "        history[\"train_policy_top3_acc\"].append(train_policy_top3_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Train Epoch Summary: Loss={train_loss:.4f}, Policy Loss={train_policy_loss:.4f}, \"\n",
    "            f\"Value Loss={train_value_loss:.4f}, Top-1 Acc={train_policy_acc:.2f}%, \"\n",
    "            f\"Top-3 Acc={train_policy_top3_acc:.2f}%, \"\n",
    "            f\"Value Range=[{train_value_min:.4f}, {train_value_max:.4f}]\"\n",
    "        )\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_policy_loss, val_value_loss = 0, 0, 0\n",
    "        val_correct, val_top3_correct, val_total = 0, 0, 0\n",
    "        val_value_min, val_value_max = float(\"inf\"), float(\"-inf\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, policy_targets, value_targets in val_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                policy_targets = policy_targets.to(\n",
    "                    device, non_blocking=True, dtype=torch.long\n",
    "                )\n",
    "                value_targets = value_targets.to(device, non_blocking=True)\n",
    "\n",
    "                policy_pred, value_pred = model(inputs)\n",
    "                policy_loss = criterion_policy(policy_pred, policy_targets)\n",
    "                value_loss = criterion_value(\n",
    "                    value_pred.squeeze(), value_targets.squeeze()\n",
    "                )\n",
    "                loss = 0.4 * policy_loss + 0.6 * value_loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_policy_loss += policy_loss.item()\n",
    "                val_value_loss += value_loss.item()\n",
    "\n",
    "                _, predicted = torch.max(policy_pred, 1)\n",
    "                val_correct += (predicted == policy_targets).sum().item()\n",
    "                _, top3_predicted = torch.topk(policy_pred, k=3, dim=1)\n",
    "                val_top3_correct += sum(\n",
    "                    policy_targets[i] in top3_predicted[i]\n",
    "                    for i in range(policy_targets.size(0))\n",
    "                )\n",
    "                val_total += policy_targets.size(0)\n",
    "\n",
    "                val_value_min = min(val_value_min, value_pred.min().item())\n",
    "                val_value_max = max(val_value_max, value_pred.max().item())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_policy_loss /= len(val_loader)\n",
    "        val_value_loss /= len(val_loader)\n",
    "        val_policy_acc = 100 * val_correct / val_total\n",
    "        val_policy_top3_acc = 100 * val_top3_correct / val_total\n",
    "\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_policy_loss\"].append(val_policy_loss)\n",
    "        history[\"val_value_loss\"].append(val_value_loss)\n",
    "        history[\"val_policy_acc\"].append(val_policy_acc)\n",
    "        history[\"val_policy_top3_acc\"].append(val_policy_top3_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Validation Summary: Loss={val_loss:.4f}, Policy Loss={val_policy_loss:.4f}, \"\n",
    "            f\"Value Loss={val_value_loss:.4f}, Top-1 Acc={val_policy_acc:.2f}%, \"\n",
    "            f\"Top-3 Acc={val_policy_top3_acc:.2f}%, \"\n",
    "            f\"Value Range=[{val_value_min:.4f}, {val_value_max:.4f}]\"\n",
    "        )\n",
    "\n",
    "        # Save model for each epoch\n",
    "        epoch_save_path = f\"checkpoints/epoch{epoch+1}-training2.pth\"\n",
    "        torch.save(model.state_dict(), epoch_save_path)\n",
    "        print(f\"Saved model for epoch {epoch+1} to {epoch_save_path}\")\n",
    "\n",
    "        # Track best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\n",
    "                f\"Saved best model with validation loss {val_loss:.4f} to {save_path}\"\n",
    "            )\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    training_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} minutes\")\n",
    "\n",
    "    # Plotting\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(\"Total Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, history[\"train_policy_loss\"], label=\"Train Policy Loss\")\n",
    "    plt.plot(epochs, history[\"val_policy_loss\"], label=\"Val Policy Loss\")\n",
    "    plt.title(\"Policy Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, history[\"train_value_loss\"], label=\"Train Value Loss\")\n",
    "    plt.plot(epochs, history[\"val_value_loss\"], label=\"Val Value Loss\")\n",
    "    plt.title(\"Value Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, history[\"train_policy_acc\"], label=\"Train Top-1 Acc\")\n",
    "    plt.plot(epochs, history[\"val_policy_acc\"], label=\"Val Top-1 Acc\")\n",
    "    plt.title(\"Policy Top-1 Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epochs, history[\"train_policy_top3_acc\"], label=\"Train Top-3 Acc\")\n",
    "    plt.plot(epochs, history[\"val_policy_top3_acc\"], label=\"Val Top-3 Acc\")\n",
    "    plt.title(\"Policy Top-3 Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_plots-3-stockfish.png\")\n",
    "    print(\"Saved training plots to 'training_plots-3-stockfish.png'\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b71823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved data shapes\n",
    "with h5py.File(\"dataset-1-6M.h5\", \"r\") as f:\n",
    "    print(f\"Inputs Shape: {f['inputs'].shape}\")\n",
    "    print(f\"Policies Shape: {f['policies'].shape}\")\n",
    "    print(f\"Values Shape: {f['values'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, and optimizer\n",
    "torch.cuda.empty_cache()\n",
    "model = RLModel(config.INPUT_SHAPE, config.OUTPUT_SHAPE).to(config.DEVICE)\n",
    "model.load_state_dict(torch.load('./latest.pth', map_location=config.DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path = \"chess_dataset-1M.h5\"\n",
    "train_model(model, hdf5_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
